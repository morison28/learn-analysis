
## 目次

- 1.機械学習の解釈性とは
- 2.線形回帰モデルを通して「解釈性」を理解する
- 3.特徴量の重要度を知る(Permutation Feature Importance)
- 4.特徴量と予測値の関係を知る(Partial Dependence)
- 5.インスタンスごとの異質性をとらえる(Individual Conditional Expectation)
- 6.予測の理由を考える(Shapley Additive exPlanations)

## 1.機械学習の解釈性とは
- MLモデルは高い予測精度を持つが，その一方でモデルの解釈性が低いという欠点を併せ持っている．実務においては分析者自身がモデルの振る舞いを把握し，説明責任を果たすことが求められる．
- 線形回帰は従来の統計モデルという位置づけ．シンプルな関係性を仮定していることもあり，モデルの振る舞いに対する透明性が高いといえる．これは解釈性が高い状態といえる．
- MLモデルに解釈性を持たせる手法として本書で紹介されているのは以下
    - `PFI: Permutation Feature Importance`
        - 予測モデルにとってどの特徴量が重要かを知る
    - `PD: Partial Dependence`
        - 特徴量とモデルの予測値の平均的な関係を知る
    - `ICE: Individual Conditional Expectation`
        - 個別のインスタンスに対して特徴量と予測値の関係を知る
    - `SAEP: Shapley Additive exPlanations`
        - モデルがなぜそのような予測値を出しているのかを知る
- PFI→PD→ICE→SAEPの順でよりミクロな視点でモデルを解釈する方法らしい(ちゃんと勉強しないとまだわからん)
- モデルの使い方
    - 弱い使い方(比較的安全)
        - モデルのデバッグ
            - 事前知識と整合的か，想定外の挙動はないか
        - モデルの振る舞いを解釈
            - モデルAは特徴量Aを重視している，特徴量Aを大きくなると予測値が大きくなる
            - モデルの一側面を捉えているだけなので間違えた解釈になる可能性はある
    - 強い使い方(注意が必要)
        - 因果関係の探索
            - モデルの振る舞いを因果関係として解釈
            - 実験やより厳密な因果推論の手法をあわせて使うべきである

## 2.線形回帰モデルと通して「解釈性」を理解する
- 内容
    - 線形回帰モデルの紹介
    - 線形回帰の解釈方法の紹介
    - ランダムフォレストとの予測精度の比較
    - ブラックモデルの解釈手法が有用であることを論じる
- 線形モデルの解釈性とは
    - 特徴量と予測値の平均的な関係が解釈できる
        - 回帰係数を見れば，特徴量1単位変動したときの予測値に与える影響度わかるよねって話
        - これは回帰係数が特徴量とモデルの予測値の(インスタンスごとではない)平均的な関係を解釈しているといえる
    - 特徴量と予測値のインスタンスごとの関係が解釈できる
        - モデルを対象の変数で偏微分すれば，対象変数1単位の変動が予測値に与える影響がわかる．
        - これは特徴量とモデルの予測値のインスタンスごとの関係を解釈しているといえる
    - 特徴量の重要度が解釈できる
        - 回帰係数は変数1単位の変動が予測値にあたえる影響なので，変数の重要度として解釈できる
        - ※ ただし，標準化でスケールを揃える必要はある
    - インスタンスごとの予測の理由が解釈できる
        - インスタンスごとに値を導入すれば，そのインスタンスの予測値がどの特徴量によってどの程度説明されているかがわかる
- 実装
    - 気が向いたらやろう
- 線形回帰の利点と注意点
    - 利点
        - 解釈性が高い
        - 長年研究されてきたので情報が豊富
        - 学習が速い
    - 注意点
        - 線形性を仮定するため，他のモデルに比べて予測精度が相対的に低い
        - 交互作用や非線形な特徴量を加工することはできるが，解釈性は下がる
- ☆3章以降では，ブラックボックスモデルにも線形回帰モデル同様の解釈性をブラックボックスモデルでも利用する手法について紹介していく

## 3.特徴量の重要度を知る(Permutation Feature Importance)
- 特徴量重要度を知る意義
    - 特徴量重要度がドメイン知識に沿っているかを確認することでモデルのデバッグを行う
    - 重要度が想定と違った振る舞いをしているとき，データ処理やモデリングにバグがある可能性に気づける
    - また，KPIを改善するための施策を考えるというシチュエーションでは，操作可能な特徴量の中で重要度の高い特徴量に介入を加えることで，より効率よくKPI改善できる可能性が高まる
- 線形回帰においては，変数ごとに回帰係数を出力すればよい
- **PFI**
    - ブラックボックスなモデルではどうやって特徴量の重要度を計算するか
    - どんなブラックボックスモデルにも共通してりようできる特徴量重要度の計算方法がある．それがPFI
    - ざっくりいうと，ひとつひとるの特徴量に対して，その特徴量の情報が「使えない」場合の予測誤差を計算し，すべての特徴量が「使える」場合の予測後誤差を比較する．予測誤差が大きく増加するなら，その特徴量は重要であるといえる
    - PFIは，重要度を測りたい変数の値シャッフルして誤差の増分を計算する
    - 変数をシャッフルするのではなく，インプットから外す手法もある(LOCOFI：Leave-One-Covariate-Out Feature Importance)
        - PFIを使うのがベターだとされている
        - LOCOFIは，特徴量ごとにモデルを学習するので遅い．PFIはインプットの変数いじるだけなのでモデリング自体は1回やればよい
        - あと，LOCOFIは運用するモデルとは異なるモデルで評価を行うことになる．
    - GPFI
        - 相関する変数をまとめてシャッフルする方法
        - ex. 「明日の売上」を予測するモデルにおいて，「今日の売上
        」と「昨日の売上」が相関している場合，重要度が分散するかもしれない．これらはまとめてシャッフルすることで，重要度を正しく評価する
        - あとは，緯度経度を位置情報としてうまいことまめたり
        - OHE変数をまとめたりするのも有効かも
- 特徴量重要度を因果関係として解釈して良さそうか
    - 疑似相関に注意するひつようがあり，因果推論の手法を使ったほうがよい
- train, testのどっちで評価するべきか？
    - まー，testでいいんじゃないってかんじ
    - モデルがオーバーフィットしていると問題だが，それはモデル自体の問題な感じもする
- PFIのいいところ
    - どんなモデルでもこの手法が使える
- PFIの注意点
    - 強く相関する変数があると，重要度の食い合いが発生する．この場合は特徴量を1つのグループにまとめてシャッフルすることで解決できる

## 4.特徴量と予測値の関係を知る(Partial Dependence)
- 特徴量重要度ではわからないこと
    - ある特徴量が大きくなると，モデルはより大きい値を予測するようになるのか．それとも小さい値を予測するようになるのか